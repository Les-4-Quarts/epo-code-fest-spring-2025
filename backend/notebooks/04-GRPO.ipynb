{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57f23c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoModelForCausalLM\n",
    "from datasets import Dataset \n",
    "\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9070f428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1326 items from dataset_labeled.jsonl\n",
      "Loaded 3000 items from dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the labeled dataset (solutions)\n",
    "labeled_items = []\n",
    "with open(\"../src/ai/testsets/dataset_labeled.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip(): # Make sure the line is not empty\n",
    "            labeled_items.append(json.loads(line.strip()))\n",
    "\n",
    "# Load the descriptions dataset\n",
    "description_items = []\n",
    "with open(\"../src/ai/testsets/dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip(): # Make sure the line is not empty\n",
    "            description_items.append(json.loads(line.strip()))\n",
    "\n",
    "# Load the prompt template\n",
    "with open(\"sdg_label_prompt.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "print(f\"Loaded {len(labeled_items)} items from dataset_labeled.jsonl\")\n",
    "print(f\"Loaded {len(description_items)} items from dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "015eb2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The number of items in labeled_items and description_items is different!\n",
      "Will process up to the length of the shorter list.\n",
      "Processed 1326 items for the dataset.\n",
      "{'solution': '3', 'prompt': 'A conversation between User and Assistant. The user provides a text, and the Assistant classifies it \\naccording to one or more of the 17 Sustainable Development Goals (SDGs). The Assistant \\nfirst thinks about the text and the different SDGs, detailing its reasoning process in relation to the input text, and then provides the user with the SDG classification(s). \\nThe reason and the sdg answer are enclosed within <reason> </reason> and <sdg> </sdg> tags, respectively. \\nThe Assistant must identify the most relevant SDG(s) for the given text. If multiple SDGs are relevant, they can all be listed. The reasoning should clearly justify the choice(s).\\\\n\\\\n\\nHere are the 17 Sustainable Development Goals (SDGs) and their descriptions:\\\\n\\n1.  **SDG 1: No Poverty:** End poverty in all its forms everywhere.\\\\n\\n2.  **SDG 2: Zero Hunger:** End hunger, achieve food security and improved nutrition and promote sustainable agriculture.\\\\n\\n3.  **SDG 3: Good Health and Well-being:** Ensure healthy lives and promote well-being for all at all ages.\\\\n\\n4.  **SDG 4: Quality Education:** Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all.\\\\n\\n5.  **SDG 5: Gender Equality:** Achieve gender equality and empower all women and girls.\\\\n\\n6.  **SDG 6: Clean Water and Sanitation:** Ensure availability and sustainable management of water and sanitation for all.\\\\n\\n7.  **SDG 7: Affordable and Clean Energy:** Ensure access to affordable, reliable, sustainable and modern energy for all.\\\\n\\n8.  **SDG 8: Decent Work and Economic Growth:** Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all.\\\\n\\n9.  **SDG 9: Industry, Innovation and Infrastructure:** Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation.\\\\n\\n10. **SDG 10: Reduced Inequalities:** Reduce inequality within and among countries.\\\\n\\n11. **SDG 11: Sustainable Cities and Communities:** Make cities and human settlements inclusive, safe, resilient and sustainable.\\\\n\\n12. **SDG 12: Responsible Consumption and Production:** Ensure sustainable consumption and production patterns.\\\\n\\n13. **SDG 13: Climate Action:** Take urgent action to combat climate change and its impacts.\\\\n\\n14. **SDG 14: Life Below Water:** Conserve and sustainably use the oceans, seas and marine resources for sustainable development.\\\\n\\n15. **SDG 15: Life on Land:** Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss.\\\\n\\n16. **SDG 16: Peace, Justice and Strong Institutions:** Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels.\\\\n\\n17. **SDG 17: Partnerships for the Goals:** Strengthen the means of implementation and revitalize the global partnership for sustainable development.\\n18. **nothing:**  the text is not related to SDG\\nThe text to be classified is:\\n[A replaceable cartridge assembly (12) for a surgical stapling apparatus, the replaceable cartridge assembly comprising: a cartridge body (124) configured to support a plurality of surgical staples; and a connector assembly (160) secured to the cartridge body, the connector assembly including: a connector housing (162) having a base portion (164) and an extension portion (166) defining first and second recesses (163, 165; 163\\', 165\\'), first and second contact members (182, 184; 192, 194) disposed within the respective first and second recesses, and a seal member (168) supported on the housing and defining first and second apertures (167, 169) providing access to the first and second recesses. 1: The present disclosure relates to surgical stapling instruments including a replaceable cartridge assembly having an identification chip. More particularly, the present disclosure relates to electrical connectors for connecting the identification chip of a cartridge assembly of a surgical stapling instrument with a handle assembly of the stapling instrument. 2: Surgical stapling instruments for applying rows of surgical staples to tissue are well known in the art and typically include a powered handle assembly, a body portion extending distally from the handle assembly, and a stapling end effector that is supported on a distal end of the body portion and is articulable relative to the body portion. The stapling end effector includes first and second jaws which are movable in relation to each other between spaced and approximated positions. The first jaw supports an anvil assembly and the second jaw supports a cartridge assembly. The cartridge assembly may be replaceable to permit reuse of the end effector during a surgical procedure. The replaceable cartridge assembly may be provided in a variety of configurations for use on tissue having different properties, e.g., thickness, density. For example, the different cartridge assemblies may have different size staples and/or different configurations of staples. 3: Many cartridge assemblies include an identification chip that is electrically coupled to the handle assembly by a conductor extending through the body portion of the surgical stapling instrument to ensure the handle assembly is programmed to operate with the attached cartridge assembly. During attachment of the cartridge assembly to the surgical stapling instrument improper loading of the cartridge assembly may result in damage to the electrical connection between the cartridge assembly and the surgical stapling instrument. Similarly, exposure of the electrical connection to fluids during a surgical procedure may damage and significantly affect operation of the surgical stapling instrument. To prevent damage to the electrical connections during loading of the cartridge assembly and during use of the surgical stapling instrument, it would be beneficial to provide a cartridge assembly with improved electrical connections. 4: Accordingly, a surgical stapling instrument including a connector mechanism is provided. The surgical stapling instrument includes a proximal body portion configured for operable engagement with a handle assembly and a tool assembly extending from the proximal body portion. The tool assembly includes an anvil assembly, a jaw member pivotally secured relative to the anvil assembly, and a first connector assembly disposed within the jaw member. The first connector assembly includes a first connector housing and first and second connector members extending from the first connector housing. The surgical stapling instrument further includes a cartridge assembly selective receivable within the jaw member of the tool assembly. The cartridge assembly includes a second connector assembly configured for electrical connection with the first connector assembly when the cartridge assembly is received within the jaw member. The second connector assembly includes a second connector housing and first and second contact members disposed within the second connector housing. The first and second connector members of the first connector assembly are configured to engage the respective first and second contact members of the second connector housing when the cartridge assembly is received within the jaw member of the tool assembly. 5: In embodiments, the cartridge assembly includes a cartridge body and a support plate. The surgical stapling instrument may further include a handle assembly operably secured to the proximal body portion. The first and second connector assemblies may form a connector mechanism for electrically connecting the cartridge assembly with the handle assembly. The first and second connector members may each include a beveled leading edge. 6: In some embodiments, the first and second contact members each includes a coil spring. Alternatively, the first and second contact members may each include a leaf spring. Each of the leaf springs may include an inclined engagement surface. Each of the leaf springs may be configured to be deflected inward upon engagement with the respective first or second connector member. 7: The second connector assembly of the cartridge assembly may include a seal member support on the second connector housing. The seal member may include first and second apertures for receiving the first and second connector members of the first connector assembly. The seal member may provide a cleaning action when the first and second connector members are received through the first and second apertures. The cleaning action may be a squeegee or wiping action. 8: The first connector assembly may be secured to the jaw member by a flange and post connection. The second connector assembly may include an identification chip. Specifications of the cartridge assembly may be loaded on the identification chip. 9: Various embodiments of the connector mechanism of the present disclosure are described herein with reference to the drawings, wherein: FIG. 1 is a perspective view of a surgical stapling instrument including a loading unit having a connector mechanism according to an embodiment of the present disclosure; FIG. 2 is a side, perspective view of the loading unit of the surgical stapling instrument shown in FIG. 1 ; FIG. 3 is a front, perspective view of the loading unit shown in FIG. 2 with a cartridge assembly of the loading unit, including a shipping wedge, prior to being loaded into a jaw member of the loading unit; FIG. 4 is a side perspective end view of the cartridge assembly shown in FIG. 3 ; FIG. 5 is an enlarged view of the indicated area of detail shown in FIG. 3 including a first connector assembly; FIG. 6 is an enlarged view of the indicated area of detail shown in FIG. 4 including a second connector assembly; FIG. 7 is a side perspective view of the first connector assembly shown in FIG. 3 secured within the jaw member of the loading unit shown in FIG. 2 ; FIG. 8 is an enlarged view of the indicated area of detail shown in FIG. 7 ; FIG. 9 is a side perspective view of the first connector assembly and a proximal portion of the jaw member shown in FIG. 7 with parts separated; FIG. 10 is a side perspective view of the first connector assembly shown in FIG. 9 ; FIG. 11 is a perspective view of a second connector assembly and a proximal portion of the cartridge assembly shown in FIG. 4 ; FIG. 12 is a perspective view of the second connector assembly and the proximal portion of the cartridge assembly shown in FIG. 11 with parts separated; FIG. 13 is an end perspective view of a second connector assembly according to an embodiment of the present disclosure; FIG. 14 is an end perspective view of a second connector assembly according to another embodiment of the present disclosure; FIG. 15 is a cross-sectional view taken along section line 15-15 of FIG. 14 ; FIG. 16 is a side perspective view of the loading unit shown in FIG. 2 including the first connector assembly shown in FIG. 3 and the second connector assembly shown in FIG. 4 and an anvil assembly shown in phantom; FIG. 17 is a cross-sectional end view of the second connector assembly shown in FIG. 13 taken along line 17-17 shown in FIG. 16 ; and FIG. 18 is a cross-sectional end view of the second connector assembly taken along line 18-18 shown in FIG. 16 . 10: Embodiments of the presently disclosed connector mechanism for a surgical stapling instrument will now be described in detail with reference to the drawings wherein like reference numerals designate identical or corresponding elements in each of the several views. In this description, the term \"proximal\" is generally used to refer to the portion of the instrument that is closer to a clinician, while the term \"distal\" is generally used to refer to the portion of the instrument that is farther from the clinician. 11: The embodiments of the present disclosure address an improved connection between a replaceable cartridge assembly and a tool assembly of a surgical stapling instrument. 12: FIG. 1 illustrates a surgical stapling instrument including a connector mechanism according to an exemplary embodiment of the present disclosure shown generally as surgical stapling instrument 10. Although illustrated as a surgical stapling instrument, aspects of the present disclosure may be modified for use with other types of end effectors that include a replaceable unit that electrically communicates with a handle assembly. The surgical stapling instrument 10 includes a powered handle assembly 12, an adapter assembly 14, and a loading unit 100. The handle assembly 12 and the adapter assembly 14 are configured to effect operation of the loading unit 100. For a detailed description of the structure and function of exemplary handle and adapter assemblies, please refer to commonly owned U.S. Pat No. 9,055,943 (\"the \\'943 Patent\"), the content of which is incorporated by reference herein in its entirety. Although the loading unit 100 is shown and described as being selectively secured to the adapter assembly 14 of the surgical stapling instrument 10, it is envisioned that the loading unit 100 may be supported directly on the distal end of the adapter assembly 14, and/or supported directly by the handle assembly 12. 13: Referring to FIGS. 1 and 2 , the loading unit 100 includes a proximal body portion 102 and a tool assembly 104. A mounting assembly 106 is secured to the tool assembly 104 and is pivotally coupled to the proximal body portion 102 of the loading unit 100 to pivotally secure the tool assembly 104 to the proximal body portion 102. The loading unit 100 is substantially as described in U.S. Pat. App. Pub. No. 2016/0249929 (\"the \\'929 Publication\"). The \\'929 Publication is hereby incorporated by reference herein in its entirety. Accordingly, the loading unit 100 will only be described to the extent necessary to fully disclose the aspects of the present disclosure. 14: With reference to FIGS. 3-6 , the tool assembly 104 of the loading unit 100 includes an anvil assembly 108 and a replaceable cartridge assembly 120 receivable within a jaw member 110 of the tool assembly 100. The anvil assembly 108 and the jaw member 110 are movable in relation to each other between an open position ( FIG. 2 ) and an approximated position ( FIG. 16 ). The cartridge assembly 120 includes a support plate 122 and a cartridge body 124 securely received within the support plate 122. The cartridge body 124 supports a plurality of staples (not shown). A shipping member 126 is releasably secured to the cartridge body 124 to retain the staples within the cartridge body 124 and/or to prevent premature firing of the loading unit 100. 15: The cartridge body 124 and the support plate 122 are attached to the jaw member 110 by a snap-fit connection as described in the \\'943 Patent, which has been incorporated herein by reference. Other forms of securing the cartridge assembly 120 to the jaw member 110 are contemplated and may be used in place of the snap-fit connection or in addition thereto. 16: The tool assembly 104 of the loading unit 100 includes a connector mechanism 130 ( FIG. 16 ) having a first or male connector assembly 140 disposed within the jaw member 110 ( FIG. 5 ) and a second or female connector assembly 160 ( FIG. 6 ) secured to the cartridge assembly 120. As will be described in further detail below, when the cartridge assembly 120 is loaded into the jaw member 110 of the loading unit 100, the second connector assembly 160 of the connector mechanism 130 electrically couples with the first connector assembly 140 of the connector mechanism 130 to provide specifications of the cartridge assembly 120 to the handle assembly 12 ( FIG. 1 ) of the surgical stapling instrument 10, e.g., staple size, number and/or configuration. 17: Turning now to FIGS. 7-9 , the jaw member 110 of the tool assembly 106 ( FIG. 1 ) of the loading unit 100 defines a channel 111 for receiving the cartridge assembly 120 ( FIG. 4 ). The jaw member 110 includes a flange 112 ( FIG. 9 ) extending within the channel 111. As will be described in further detail below, a tab 112a ( FIG. 8 ) on a free end of the flange 112 is configured to secure the first connector assembly 140 of the connector mechanism 130 ( FIG. 15 ) within the channel 111. The jaw member 110 defines an opening 113 ( FIG. 9 ) disposed adjacent to the flange 112 for receiving a boss 144 ( FIG. 10 ) of the first connector assembly 140. 18: With additional reference to FIG. 10 , the first connector assembly 140 of the connector mechanism 130 includes a first connector housing 142. The first connector housing 142 defines a slot 143 through which the flange 112 of the jaw member 110 of the tool assembly 106 is received. As shown in FIG. 8 , when the flange 112 of the jaw member 110 is received through the slot 143 in the first connector housing 142, a tab 112a formed or supported on the free end of the flange 112 engages the first connector housing 142 to secure the first connector assembly 140 to the jaw member 110. In this manner, the first connector assembly 140 is secured to the jaw member 110 with a snap retention. The first connector housing 142 includes the boss 144. As noted above, the boss 144 is configured to be received within the opening 113 ( FIG. 9 ) in the jaw member 110 when the flange 112 is received within the slot 143. Receipt of the boss 144 of the first connector housing 142 in the opening 113 in the jaw member 110 provides additional torque resistance for the first connector assembly 140 during loading of the cartridge assembly 120 within the jaw member 110. 19: Although shown with the first connector assembly 140 being secured to the jaw member 110 of the tool assembly 106 by the flange 112 and opening 113, it is envisioned that the first connector assembly 140 may be secured to the jaw member 110 in any suitable manner. For example, the first connector assembly 140 may be heat staked, bonded, welded, or otherwise secured to the jaw member 110. 20: With continued reference to FIG. 10 , first and second connector members 146, 148 extend outwardly from the first connector housing 142 and are positioned and configured to engage the second connector assembly 160 ( FIG. 6 ) of the connector mechanism 130 when the cartridge assembly 120 is received within the channel 111 of the jaw member 110. In embodiments, and as shown, a leading portion 146a, 148a of each of the first and second connector members 146, 148, respectively, are beveled to facilitate engagement of the first connector assembly 140 with the second connector assembly 160 of the connector mechanism 130. In embodiments, the first and second connector members 146, 148 measure about 0.05 inches by 0.015 inches in size and are formed by folding 0.0075 inch thick sheet metal. In embodiments, the first and second connector members 146, 148 are be spaced 0.050 inches apart. Alternately, other dimensional characteristics are envisioned. 21: With reference to FIGS. 11 and 12 , the cartridge body 124 of the cartridge assembly 120 includes a projection 126 extending from a proximal portion 124a of the cartridge body 124. The projection 126 defines a recess 127 for receiving the second connector assembly 160 of the connector mechanism 130. The projection 126 further defines a notch 127a opposite the open end of the recess 127. The notch 127a is configured to receive a tab 166a of the second connector housing 162 to secure the second connector housing 162 to the cartridge body 124. The projection 126 is configured such that when the second connector assembly 160 is received within the recess 127 and the cartridge assembly 120 is loaded into the jaw member 110, the second connector assembly 160 engages the first connector assembly 140 that is secured to the jaw member 110. 22: The second connector assembly 160 includes a second connector housing 162 having a base portion 164 and an extension portion 166 and defines first and second recesses 163, 165 ( FIG. 15 ). As will be described in further detail below, first and second contact members, e.g., coil springs 182, 184 ( FIG. 13 ) or leaf springs 192, 194 ( FIG. 15 ), are operably disposed within the respective first and second recesses 163, 165 ( FIG. 13 ) or 163\\', 165\\' ( FIG. 15 ). 23: A seal member 168 may be secured to the base portion 164 of the second connector housing 162 of the second connector assembly 160. In embodiments, and as shown, the seal member 168 ( FIG. 12 ) is over molded to the base portion 164 of the second connector housing 162. Alternatively, the seal member 168 may include a perimeter bead, e.g., O-ring or the like, that is adhered or otherwise secured to the connector housing 162. The seal member 168 defines first and second apertures 167, 169 for providing access to the first and second contact members, e.g., coil springs 182, 184 ( FIG. 13 ) or leaf springs 192, 194 ( FIG. 15 ), respectively, received within the first and second recesses 163, 165]\\n\\nPlease identify the most relevant SDG(s) for this text, providing your reasoning and classification in the specified format. The reason and the sdg answer are enclosed within <reason> </reason> and <sdg> </sdg> tags, respectively'}\n"
     ]
    }
   ],
   "source": [
    "prepared_data = []\n",
    "\n",
    "# Assuming labeled_items and description_items correspond line by line\n",
    "# and are of the same length.\n",
    "if len(labeled_items) != len(description_items):\n",
    "    print(\"Warning: The number of items in labeled_items and description_items is different!\")\n",
    "    print(\"Will process up to the length of the shorter list.\")\n",
    "\n",
    "for i in range(min(len(labeled_items), len(description_items))):\n",
    "    label_item = labeled_items[i]        # e.g., {\"patent_text_key\": \"This is the solution.\", \"reason\": \"...\"}\n",
    "    desc_item = description_items[i]     # e.g., {\"patent_text_key\": \"Full description for prompt.\", \"other_field\": \"...\"}\n",
    "\n",
    "    current_solution = None\n",
    "    description_for_prompt = None\n",
    "    \n",
    "    # Find the solution and the key that provided it\n",
    "    key_for_solution_and_description = None\n",
    "    for key, value in label_item.items():\n",
    "        if key != \"reason\":\n",
    "            current_solution = value\n",
    "            key_for_solution_and_description = key\n",
    "            break # Found the first non-reason item\n",
    "\n",
    "    if current_solution is not None and key_for_solution_and_description is not None:\n",
    "        # Now use this key_for_solution_and_description to get the description from desc_item\n",
    "        description_for_prompt = desc_item.get(key_for_solution_and_description)\n",
    "\n",
    "        if description_for_prompt is not None:\n",
    "            final_prompt = prompt_template.replace(\"{description}\", description_for_prompt)\n",
    "            prepared_data.append({\n",
    "                'solution': current_solution,\n",
    "                'prompt': final_prompt\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: For item index {i}, solution found with key '{key_for_solution_and_description}', but this key was not found in the corresponding description item: {desc_item}\")\n",
    "    else:\n",
    "        print(f\"Warning: For item index {i}, no suitable solution key (non-'reason') found in label_item: {label_item}\")\n",
    "\n",
    "print(f\"Processed {len(prepared_data)} items for the dataset.\")\n",
    "print(prepared_data[0])\n",
    "train_dataset = Dataset.from_list(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c55f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-1.7B-FP8\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7445f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,146,880 || all params: 597,196,800 || trainable%: 0.1920\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50af5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<sdg>.*?</sdg>\\s*<reason>.*?</reason>$\"\n",
    "    print(completions)\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, content) for content in completion_contents]\n",
    "    rewards_list = [1.0 if match else 0.0 for match in matches]\n",
    "    return rewards_list\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<sdg>.*?</sdg>\\s*<reason>.*?</reason>$\"\n",
    "    \n",
    "    # 'completions' is already the list of strings you want to check.\n",
    "    # So, 'completion_contents' can just be 'completions'.\n",
    "    # Each 'content' in the next line will be one of the generated strings.\n",
    "    print(f\"Received completions: {completions}\") # This print is helpful for debugging\n",
    "\n",
    "    # Directly use each completion string for matching\n",
    "    matches = [re.match(pattern, single_completion_string) for single_completion_string in completions]\n",
    "    rewards_list = [1.0 if match else 0.0 for match in matches]\n",
    "    \n",
    "    print(f\"Generated rewards: {rewards_list}\") # Optional: to see the rewards\n",
    "    return rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65bab275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdg_reason(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"Extracts SDG (Sustainable Development Goal) and reason from a text.\n",
    "\n",
    "    The input text is expected to contain <sdg> and <reason> XML-like tags\n",
    "    enclosing the relevant information.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string, potentially containing SDG and reason\n",
    "            information enclosed in <sdg>...</sdg> and <reason>...</reason>\n",
    "            tags.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing two strings:\n",
    "            - The content of the <sdg> tag, stripped of leading/trailing\n",
    "              whitespace.\n",
    "            - The content of the <reason> tag, stripped of leading/trailing\n",
    "              whitespace.\n",
    "            Returns empty strings for either if the corresponding tag is not found.\n",
    "    \"\"\"\n",
    "    reason_match = re.search(r'<reason>(.*?)</reason>', text, re.DOTALL)\n",
    "    sdg_match = re.search(r'<sdg>(.*?)</sdg>', text, re.DOTALL)\n",
    "\n",
    "    reason_content_regex = \"\"\n",
    "    sdg_content_regex = \"\"\n",
    "\n",
    "    if reason_match:\n",
    "        reason_content_regex = reason_match.group(1).strip()\n",
    "\n",
    "    if sdg_match:\n",
    "        sdg_content_regex = sdg_match.group(1).strip()\n",
    "    return sdg_content_regex, reason_content_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9307f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sdgs(text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extracts and standardizes SDG references from a given text.\n",
    "\n",
    "        This method identifies SDG mentions in various formats, including:\n",
    "        - \"SDG\" followed by a number (e.g., \"SDG1\", \"SDG 2\").\n",
    "        - Numbers with sub-targets (e.g., \"16.1\", \"3.4\"), where the main number\n",
    "          is extracted.\n",
    "        - Standalone numbers (1-17) that appear at the beginning of the text\n",
    "          or are preceded by common delimiters (commas, semicolons, colons, whitespace)\n",
    "          and followed by delimiters or the end of the string.\n",
    "        The matching is case-insensitive.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to scan for SDG references.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of unique SDGs found, formatted as \"SDG<number>\"\n",
    "                (e.g., [\"SDG1\", \"SDG2\"]), sorted numerically. Returns [\"None\"]\n",
    "                if no valid SDGs (1-17) are found or if the input text is empty\n",
    "                or not a string.\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            # Modified to return [\"None\"] as per original logic for empty/invalid text\n",
    "            return [\"None\"]\n",
    "\n",
    "        sdg_numbers = set()  # Use set to avoid duplicates\n",
    "\n",
    "        # Pattern 1: SDG followed by number with optional sub-target\n",
    "        # Captures: SDG1, sdg 2, SDG13.4, etc.\n",
    "        sdg_pattern = r'(?i)\\bsdg\\s*(\\d{1,2})(?:\\.\\d+)?\\b'\n",
    "        sdg_matches = re.findall(sdg_pattern, text)\n",
    "        for match in sdg_matches:\n",
    "            number = int(match)\n",
    "            if 1 <= number <= 17:\n",
    "                sdg_numbers.add(number)\n",
    "\n",
    "        # Pattern 2: Number with sub-target (e.g., \"16.1\", \"3.4\")\n",
    "        # Look for patterns like X.Y where X is 1-17\n",
    "        number_with_sub_pattern = r'\\b(\\d{1,2})\\.\\d+\\b'\n",
    "        sub_matches = re.findall(number_with_sub_pattern, text)\n",
    "        for match in sub_matches:\n",
    "            number = int(match)\n",
    "            if 1 <= number <= 17:\n",
    "                sdg_numbers.add(number)\n",
    "\n",
    "        # Pattern 3: Standalone numbers at beginning or after delimiters\n",
    "        standalone_pattern = r'(?:^|[,;:]\\s*|(?<=\\s))(\\d{1,2})(?=\\s*[,;]|\\s*$|\\s+)'\n",
    "        standalone_matches = re.findall(standalone_pattern, text.strip())\n",
    "        for match in standalone_matches:\n",
    "            number = int(match)\n",
    "            if 1 <= number <= 17:\n",
    "                sdg_numbers.add(number)\n",
    "\n",
    "        # Convert to sorted list of formatted strings\n",
    "        result = [f\"SDG{num}\" for num in sorted(sdg_numbers)]\n",
    "\n",
    "        return [\"None\"] if not result else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c97e9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(list1: List[str], list2: List[str]) -> float:\n",
    "  \"\"\"Calculates a similarity score between two lists based on common elements.\n",
    "\n",
    "  The score is defined as the ratio of the number of common unique elements\n",
    "  to the total number of unique elements across both lists (Jaccard index\n",
    "  for sets derived from the lists).\n",
    "\n",
    "  Args:\n",
    "    list1 (List[Any]): The first list of items.\n",
    "    list2 (List[Any]): The second list of items.\n",
    "\n",
    "  Returns:\n",
    "    float: The similarity score, ranging from 0.0 (no common elements)\n",
    "           to 1.0 (all unique elements are common, or both lists are\n",
    "           effectively the same in terms of unique content if order and\n",
    "           duplicates are ignored). Returns 0.0 if both lists are empty\n",
    "           or if the union of elements is empty to avoid division by zero,\n",
    "           though the formula naturally handles this if at least one list\n",
    "           is non-empty.\n",
    "  \"\"\"\n",
    "  set1 = set(list1)\n",
    "  set2 = set(list2)\n",
    "\n",
    "  # Number of unique combined elements\n",
    "  num_union = len(set1 | set2)  # Union of sets\n",
    "\n",
    "  # Number of common elements between list2 and list1\n",
    "  num_intersection = len(set1 & set2)  # Intersection of sets\n",
    "\n",
    "  if num_union == 0:\n",
    "    return 0.0  # Avoid division by zero if both lists result in empty sets\n",
    "  return num_intersection / num_union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a93c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
    "    sdg_tag_solutions = kwargs[\"solution\"]\n",
    "    print(completions)\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for content, sdg_tag_solution in zip(completion_contents, sdg_tag_solutions):\n",
    "        sdg_tag_content, _ = get_sdg_reason(content)\n",
    "\n",
    "        sdg_list = extract_sdgs(sdg_tag_content)\n",
    "        sdg_list_solution = extract_sdgs(sdg_tag_solution)\n",
    "        if len(sdg_list) != 0:\n",
    "            try:\n",
    "                rewards.append(score(sdg_list, sdg_list_solution))\n",
    "            except Exception:\n",
    "                rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "\n",
    "# Configure training arguments using GRPOConfig\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"Qwen3-1.7B-GRPO\",\n",
    "    learning_rate=1e-5,\n",
    "    remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    # Parameters that control de data preprocessing\n",
    "    max_completion_length=500,  # default: 256\n",
    "    num_generations=4,  # default: 8\n",
    "    max_prompt_length=5000,  # default: 512\n",
    "    # Parameters related to reporting and saving\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1fe526af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model, reward_funcs=[format_reward, accuracy_reward], args=training_args, train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c116e90",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 16.79 GiB is allocated by PyTorch, and 5.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/trainer.py:3739\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer.train):\n\u001b[32m   3737\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.train()\n\u001b[32m-> \u001b[39m\u001b[32m3739\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   3741\u001b[39m     loss_mb = smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/trl/extras/profiling.py:87\u001b[39m, in \u001b[36mprofiling_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func.\u001b[34m__name__\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/trl/trainer/grpo_trainer.py:899\u001b[39m, in \u001b[36mGRPOTrainer._prepare_inputs\u001b[39m\u001b[34m(self, accumulated_local_batch)\u001b[39m\n\u001b[32m    896\u001b[39m generate_every = \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps * \u001b[38;5;28mself\u001b[39m.num_iterations\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._step % generate_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffered_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    898\u001b[39m     \u001b[38;5;66;03m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m     accumulated_local_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccumulated_local_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m     \u001b[38;5;28mself\u001b[39m._buffered_inputs = split_tensor_dict(\n\u001b[32m    901\u001b[39m         accumulated_local_batch, \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n\u001b[32m    902\u001b[39m     )\n\u001b[32m    903\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m._buffered_inputs[\u001b[38;5;28mself\u001b[39m._step % \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/trl/trainer/grpo_trainer.py:974\u001b[39m, in \u001b[36mGRPOTrainer._generate_and_score_completions\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# Regular generation path\u001b[39;00m\n\u001b[32m    971\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(\n\u001b[32m    972\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped, \u001b[38;5;28mself\u001b[39m.accelerator, gather_deepspeed3_params=\u001b[38;5;28mself\u001b[39m.args.ds3_gather_for_generation\n\u001b[32m    973\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m         prompt_completion_ids = \u001b[43munwrapped_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# Compute prompt length and extract completion ids\u001b[39;00m\n\u001b[32m    979\u001b[39m     prompt_length = prompt_ids.size(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/peft/peft_model.py:1875\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1873\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1874\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1875\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1877\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3557\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3554\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3557\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:730\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    725\u001b[39m output_hidden_states = (\n\u001b[32m    726\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    727\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    743\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    744\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:463\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    461\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:284\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:213\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m input_shape = hidden_states.shape[:-\u001b[32m1\u001b[39m]\n\u001b[32m    211\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_norm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape)).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    214\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_norm(\u001b[38;5;28mself\u001b[39m.k_proj(hidden_states).view(hidden_shape)).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    215\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/peft/tuners/lora/layer.py:724\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    722\u001b[39m dropout = \u001b[38;5;28mself\u001b[39m.lora_dropout[active_adapter]\n\u001b[32m    723\u001b[39m scaling = \u001b[38;5;28mself\u001b[39m.scaling[active_adapter]\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cast_input_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_dora[active_adapter]:\n\u001b[32m    727\u001b[39m     result = result + lora_B(lora_A(dropout(x))) * scaling\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/epo-code-fest-spring-2025/backend/.venv/lib/python3.13/site-packages/peft/tuners/lora/layer.py:508\u001b[39m, in \u001b[36mLoraLayer._cast_input_dtype\u001b[39m\u001b[34m(self, x, dtype)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cast_input_dtype_enabled) \u001b[38;5;129;01mor\u001b[39;00m (x.dtype == dtype):\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 16.79 GiB is allocated by PyTorch, and 5.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3289801",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
